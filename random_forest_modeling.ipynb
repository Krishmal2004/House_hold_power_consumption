{\n  "nbformat": 4,\n  "nbformat_minor": 0,\n  "metadata": {\n    "colab": {\n      "provenance": []\n    },\n    "kernelspec": {\n      "name": "python3",\n      "display_name": "Python 3"\n    },\n    "language_info": {\n      "name": "python"\n    }\n  },\n  "cells": [\n    {\n      "cell_type": "markdown",\n      "source": [\n        "# Random Forest Implementation with Hyperparameter Tuning\n",\n        "## House Hold Power Consumption Analysis\n",\n        "\n",\n        "This notebook implements Random Forest models for all three preprocessed datasets:\n",\n        "1. preprocessed_household_power.csv\n",\n        "2. preprocessed_appliances_energy.csv\n",\n        "3. preprocessed_smart_home_energy.csv"\n      ],\n      "metadata": {\n        "id": "header_cell"\n      }\n    },\n    {\n      "cell_type": "code",\n      "execution_count": null,\n      "metadata": {\n        "id": "imports_cell"\n      },\n      "outputs": [],\n      "source": [\n        "# Import required libraries\n",\n        "import pandas as pd\n",\n        "import numpy as np\n",\n        "import matplotlib.pyplot as plt\n",\n        "import seaborn as sns\n",\n        "from sklearn.ensemble import RandomForestRegressor\n",\n        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",\n        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",\n        "import time\n",\n        "import warnings\n",\n        "warnings.filterwarnings('ignore')\n",\n        "\n",\n        "# Set visualization style\n",\n        "sns.set_style('whitegrid')\n",\n        "plt.rcParams['figure.figsize'] = (12, 6)"\n      ]\n    },\n    {\n      "cell_type": "markdown",\n      "source": [\n        "## Utility Functions"\n      ],\n      "metadata": {\n        "id": "utils_header"\n      }\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "def evaluate_model(model, X_test, y_test, dataset_name):\n",\n        "    \"\"\"\n",\n        "    Evaluate model performance and print metrics\n",\n        "    \"\"\"\n",\n        "    y_pred = model.predict(X_test)\n",\n        "    \n",\n        "    mse = mean_squared_error(y_test, y_pred)\n",\n        "    rmse = np.sqrt(mse)\n",\n        "    mae = mean_absolute_error(y_test, y_pred)\n",\n        "    r2 = r2_score(y_test, y_pred)\n",\n        "    \n",\n        "    print(f\"\\n{'='*60}\")\n",\n        "    print(f\"Model Performance for {dataset_name}\")\n",\n        "    print(f\"{'='*60}\")\n",\n        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",\n        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",\n        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",\n        "    print(f\"RÂ² Score: {r2:.4f}\")\n",\n        "    print(f\"{'='*60}\\n\")\n",\n        "    \n",\n        "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'predictions': y_pred}\n",\n        "\n",\n        "def plot_predictions(y_test, y_pred, dataset_name):\n",\n        "    \"\"\"\n",\n        "    Plot actual vs predicted values\n",\n        "    \"\"\"\n",\n        "    plt.figure(figsize=(15, 5))\n",\n        "    \n",\n        "    # Scatter plot\n",\n        "    plt.subplot(1, 3, 1)\n",\n        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",\n        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",\n        "    plt.xlabel('Actual Values')\n",\n        "    plt.ylabel('Predicted Values')\n",\n        "    plt.title(f'{dataset_name}: Actual vs Predicted')\n",\n        "    \n",\n        "    # Residual plot\n",\n        "    plt.subplot(1, 3, 2)\n",\n        "    residuals = y_test - y_pred\n",\n        "    plt.scatter(y_pred, residuals, alpha=0.5)\n",\n        "    plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",\n        "    plt.xlabel('Predicted Values')\n",\n        "    plt.ylabel('Residuals')\n",\n        "    plt.title(f'{dataset_name}: Residual Plot')\n",\n        "    \n",\n        "    # Distribution of residuals\n",\n        "    plt.subplot(1, 3, 3)\n",\n        "    plt.hist(residuals, bins=50, edgecolor='black')\n",\n        "    plt.xlabel('Residuals')\n",\n        "    plt.ylabel('Frequency')\n",\n        "    plt.title(f'{dataset_name}: Residual Distribution')\n",\n        "    \n",\n        "    plt.tight_layout()\n",\n        "    plt.show()\n",\n        "\n",\n        "def plot_feature_importance(model, feature_names, dataset_name, top_n=15):\n",\n        "    \"\"\"\n",\n        "    Plot feature importance from Random Forest\n",\n        "    \"\"\"\n",\n        "    importances = model.feature_importances_\n",\n        "    indices = np.argsort(importances)[::-1][:top_n]\n",\n        "    \n",\n        "    plt.figure(figsize=(12, 6))\n",\n        "    plt.title(f'{dataset_name}: Top {top_n} Feature Importances')\n",\n        "    plt.bar(range(top_n), importances[indices])\n",\n        "    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha='right')\n",\n        "    plt.xlabel('Features')\n",\n        "    plt.ylabel('Importance')\n",\n        "    plt.tight_layout()\n",\n        "    plt.show()\n",\n        "    \n",\n        "    # Print top features\n",\n        "    print(f\"\\nTop {top_n} Most Important Features for {dataset_name}:\")\n",\n        "    for i, idx in enumerate(indices, 1):\n",\n        "        print(f\"{i}. {feature_names[idx]}: {importances[idx]:.4f}\")"\n      ],\n      "metadata": {\n        "id": "utils_cell"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "markdown",\n      "source": [\n        "## Dataset 1: Individual Household Electric Power Consumption"\n      ],\n      "metadata": {\n        "id": "dataset1_header"\n      }\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "print(\"\\n\" + \"=\"*80)\n",\n        "print(\"DATASET 1: Individual Household Electric Power Consumption\")\n",\n        "print(\"=\"*80)\n",\n        "\n",\n        "# Load the preprocessed data\n",\n        "df1 = pd.read_csv('preprocessed_household_power.csv')\n",\n        "print(f\"\\nDataset shape: {df1.shape}\")\n",\n        "print(f\"\\nColumns: {df1.columns.tolist()}\")\n",\n        "\n",\n        "# Prepare features and target\n",\n        "# Remove datetime column if it exists\n",\n        "if 'datetime' in df1.columns:\n",\n        "    df1 = df1.drop('datetime', axis=1)\n",\n        "elif 'Unnamed: 0' in df1.columns:\n",\n        "    df1 = df1.drop('Unnamed: 0', axis=1)\n",\n        "\n",\n        "# Define target variable (Global_active_power)\n",\n        "target_col_1 = 'Global_active_power'\n",\n        "X1 = df1.drop(target_col_1, axis=1)\n",\n        "y1 = df1[target_col_1]\n",\n        "\n",\n        "# Remove any non-numeric columns\n",\n        "X1 = X1.select_dtypes(include=[np.number])\n",\n        "\n",\n        "print(f\"\\nFeatures shape: {X1.shape}\")\n",\n        "print(f\"Target shape: {y1.shape}\")\n",\n        "\n",\n        "# Split the data\n",\n        "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",\n        "    X1, y1, test_size=0.2, random_state=42\n",\n        ")\n",\n        "\n",\n        "print(f\"\\nTraining set size: {X1_train.shape[0]}\")\n",\n        "print(f\"Test set size: {X1_test.shape[0]}\")"\n      ],\n      "metadata": {\n        "id": "dataset1_load"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Baseline Random Forest Model\n",\n        "print(\"\\n--- Training Baseline Random Forest Model ---\")\n",\n        "start_time = time.time()\n",\n        "\n",\n        "rf_baseline_1 = RandomForestRegressor(\n",\n        "    n_estimators=100,\n",\n        "    random_state=42,\n",\n        "    n_jobs=-1\n",\n        ")\n",\n        "rf_baseline_1.fit(X1_train, y1_train)\n",\n        "\n",\n        "baseline_time_1 = time.time() - start_time\n",\n        "print(f\"Baseline model training time: {baseline_time_1:.2f} seconds\")\n",\n        "\n",\n        "# Evaluate baseline model\n",\n        "baseline_results_1 = evaluate_model(rf_baseline_1, X1_test, y1_test, \"Dataset 1 - Baseline\")\n",\n        "\n",\n        "# Plot predictions\n",\n        "plot_predictions(y1_test, baseline_results_1['predictions'], \"Dataset 1 - Baseline\")\n",\n        "\n",\n        "# Plot feature importance\n",\n        "plot_feature_importance(rf_baseline_1, X1.columns, \"Dataset 1 - Baseline\")"\n      ],\n      "metadata": {\n        "id": "dataset1_baseline"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Hyperparameter Tuning with RandomizedSearchCV\n",\n        "print(\"\\n--- Hyperparameter Tuning (RandomizedSearchCV) ---\")\n",\n        "\n",\n        "# Define parameter grid\n",\n        "param_dist_1 = {\n",\n        "    'n_estimators': [100, 200, 300, 500],\n",\n        "    'max_depth': [10, 20, 30, 40, None],\n",\n        "    'min_samples_split': [2, 5, 10],\n",\n        "    'min_samples_leaf': [1, 2, 4],\n",\n        "    'max_features': ['sqrt', 'log2', None],\n",\n        "    'bootstrap': [True, False]\n",\n        "}\n",\n        "\n",\n        "# Create base model\n",\n        "rf_random_1 = RandomForestRegressor(random_state=42, n_jobs=-1)\n",\n        "\n",\n        "# Randomized search\n",\n        "random_search_1 = RandomizedSearchCV(\n",\n        "    estimator=rf_random_1,\n",\n        "    param_distributions=param_dist_1,\n",\n        "    n_iter=50,\n",\n        "    cv=3,\n",\n        "    verbose=2,\n",\n        "    random_state=42,\n",\n        "    n_jobs=-1,\n",\n        "    scoring='neg_mean_squared_error'\n",\n        ")\n",\n        "\n",\n        "start_time = time.time()\n",\n        "random_search_1.fit(X1_train, y1_train)\n",\n        "tuning_time_1 = time.time() - start_time\n",\n        "\n",\n        "print(f\"\\nHyperparameter tuning time: {tuning_time_1:.2f} seconds\")\n",\n        "print(f\"\\nBest parameters: {random_search_1.best_params_}\")\n",\n        "print(f\"Best cross-validation score (neg MSE): {random_search_1.best_score_:.4f}\")"\n      ],\n      "metadata": {\n        "id": "dataset1_tuning"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Evaluate tuned model\n",\n        "print(\"\\n--- Evaluating Tuned Model ---\")\n",\n        "rf_tuned_1 = random_search_1.best_estimator_\n",\n        "\n",\n        "tuned_results_1 = evaluate_model(rf_tuned_1, X1_test, y1_test, \"Dataset 1 - Tuned\")\n",\n        "\n",\n        "# Plot predictions\n",\n        "plot_predictions(y1_test, tuned_results_1['predictions'], \"Dataset 1 - Tuned\")\n",\n        "\n",\n        "# Plot feature importance\n",\n        "plot_feature_importance(rf_tuned_1, X1.columns, \"Dataset 1 - Tuned\")\n",\n        "\n",\n        "# Compare baseline vs tuned\n",\n        "print(\"\\n--- Baseline vs Tuned Comparison (Dataset 1) ---\")\n",\n        "comparison_df_1 = pd.DataFrame({\n",\n        "    'Metric': ['MSE', 'RMSE', 'MAE', 'RÂ²'],\n",\n        "    'Baseline': [baseline_results_1['mse'], baseline_results_1['rmse'], \n",\n        "                 baseline_results_1['mae'], baseline_results_1['r2']],\n",\n        "    'Tuned': [tuned_results_1['mse'], tuned_results_1['rmse'], \n",\n        "              tuned_results_1['mae'], tuned_results_1['r2']]\n",\n        "})\n",\n        "comparison_df_1['Improvement (%)'] = ((comparison_df_1['Baseline'] - comparison_df_1['Tuned']) / \n",\n        "                                       comparison_df_1['Baseline'] * 100)\n",\n        "print(comparison_df_1.to_string(index=False))"\n      ],\n      "metadata": {\n        "id": "dataset1_eval"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "markdown",\n      "source": [\n        "## Dataset 2: Appliances Energy Prediction"\n      ],\n      "metadata": {\n        "id": "dataset2_header"\n      }\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "print(\"\\n\" + \"=\"*80)\n",\n        "print(\"DATASET 2: Appliances Energy Prediction\")\n",\n        "print(\"=\"*80)\n",\n        "\n",\n        "# Load the preprocessed data\n",\n        "df2 = pd.read_csv('preprocessed_appliances_energy.csv')\n",\n        "print(f\"\\nDataset shape: {df2.shape}\")\n",\n        "print(f\"\\nColumns: {df2.columns.tolist()}\")\n",\n        "\n",\n        "# Prepare features and target\n",\n        "if 'date' in df2.columns:\n",\n        "    df2 = df2.drop('date', axis=1)\n",\n        "elif 'Unnamed: 0' in df2.columns:\n",\n        "    df2 = df2.drop('Unnamed: 0', axis=1)\n",\n        "\n",\n        "# Define target variable (Appliances)\n",\n        "target_col_2 = 'Appliances'\n",\n        "X2 = df2.drop(target_col_2, axis=1)\n",\n        "y2 = df2[target_col_2]\n",\n        "\n",\n        "# Remove any non-numeric columns\n",\n        "X2 = X2.select_dtypes(include=[np.number])\n",\n        "\n",\n        "print(f\"\\nFeatures shape: {X2.shape}\")\n",\n        "print(f\"Target shape: {y2.shape}\")\n",\n        "\n",\n        "# Split the data\n",\n        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",\n        "    X2, y2, test_size=0.2, random_state=42\n",\n        ")\n",\n        "\n",\n        "print(f\"\\nTraining set size: {X2_train.shape[0]}\")\n",\n        "print(f\"Test set size: {X2_test.shape[0]}\")"\n      ],\n      "metadata": {\n        "id": "dataset2_load"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Baseline Random Forest Model\n",\n        "print(\"\\n--- Training Baseline Random Forest Model ---\")\n",\n        "start_time = time.time()\n",\n        "\n",\n        "rf_baseline_2 = RandomForestRegressor(\n",\n        "    n_estimators=100,\n",\n        "    random_state=42,\n",\n        "    n_jobs=-1\n",\n        ")\n",\n        "rf_baseline_2.fit(X2_train, y2_train)\n",\n        "\n",\n        "baseline_time_2 = time.time() - start_time\n",\n        "print(f\"Baseline model training time: {baseline_time_2:.2f} seconds\")\n",\n        "\n",\n        "# Evaluate baseline model\n",\n        "baseline_results_2 = evaluate_model(rf_baseline_2, X2_test, y2_test, \"Dataset 2 - Baseline\")\n",\n        "\n",\n        "# Plot predictions\n",\n        "plot_predictions(y2_test, baseline_results_2['predictions'], \"Dataset 2 - Baseline\")\n",\n        "\n",\n        "# Plot feature importance\n",\n        "plot_feature_importance(rf_baseline_2, X2.columns, \"Dataset 2 - Baseline\")"\n      ],\n      "metadata": {\n        "id": "dataset2_baseline"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Hyperparameter Tuning with RandomizedSearchCV\n",\n        "print(\"\\n--- Hyperparameter Tuning (RandomizedSearchCV) ---\")\n",\n        "\n",\n        "# Define parameter grid\n",\n        "param_dist_2 = {\n",\n        "    'n_estimators': [100, 200, 300, 500],\n",\n        "    'max_depth': [10, 20, 30, 40, None],\n",\n        "    'min_samples_split': [2, 5, 10],\n",\n        "    'min_samples_leaf': [1, 2, 4],\n",\n        "    'max_features': ['sqrt', 'log2', None],\n",\n        "    'bootstrap': [True, False]\n",\n        "}\n",\n        "\n",\n        "# Create base model\n",\n        "rf_random_2 = RandomForestRegressor(random_state=42, n_jobs=-1)\n",\n        "\n",\n        "# Randomized search\n",\n        "random_search_2 = RandomizedSearchCV(\n",\n        "    estimator=rf_random_2,\n",\n        "    param_distributions=param_dist_2,\n",\n        "    n_iter=50,\n",\n        "    cv=3,\n",\n        "    verbose=2,\n",\n        "    random_state=42,\n",\n        "    n_jobs=-1,\n",\n        "    scoring='neg_mean_squared_error'\n",\n        ")\n",\n        "\n",\n        "start_time = time.time()\n",\n        "random_search_2.fit(X2_train, y2_train)\n",\n        "tuning_time_2 = time.time() - start_time\n",\n        "\n",\n        "print(f\"\\nHyperparameter tuning time: {tuning_time_2:.2f} seconds\")\n",\n        "print(f\"\\nBest parameters: {random_search_2.best_params_}\")\n",\n        "print(f\"Best cross-validation score (neg MSE): {random_search_2.best_score_:.4f}\")"\n      ],\n      "metadata": {\n        "id": "dataset2_tuning"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Evaluate tuned model\n",\n        "print(\"\\n--- Evaluating Tuned Model ---\")\n",\n        "rf_tuned_2 = random_search_2.best_estimator_\n",\n        "\n",\n        "tuned_results_2 = evaluate_model(rf_tuned_2, X2_test, y2_test, \"Dataset 2 - Tuned\")\n",\n        "\n",\n        "# Plot predictions\n",\n        "plot_predictions(y2_test, tuned_results_2['predictions'], \"Dataset 2 - Tuned\")\n",\n        "\n",\n        "# Plot feature importance\n",\n        "plot_feature_importance(rf_tuned_2, X2.columns, \"Dataset 2 - Tuned\")\n",\n        "\n",\n        "# Compare baseline vs tuned\n",\n        "print(\"\\n--- Baseline vs Tuned Comparison (Dataset 2) ---\")\n",\n        "comparison_df_2 = pd.DataFrame({\n",\n        "    'Metric': ['MSE', 'RMSE', 'MAE', 'RÂ²'],\n",\n        "    'Baseline': [baseline_results_2['mse'], baseline_results_2['rmse'], \n",\n        "                 baseline_results_2['mae'], baseline_results_2['r2']],\n",\n        "    'Tuned': [tuned_results_2['mse'], tuned_results_2['rmse'], \n",\n        "              tuned_results_2['mae'], tuned_results_2['r2']]\n",\n        "})\n",\n        "comparison_df_2['Improvement (%)'] = ((comparison_df_2['Baseline'] - comparison_df_2['Tuned']) / \n",\n        "                                       comparison_df_2['Baseline'] * 100)\n",\n        "print(comparison_df_2.to_string(index=False))"\n      ],\n      "metadata": {\n        "id": "dataset2_eval"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "markdown",\n      "source": [\n        "## Dataset 3: Smart Home Energy Consumption"\n      ],\n      "metadata": {\n        "id": "dataset3_header"\n      }\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "print(\"\\n\" + \"=\"*80)\n",\n        "print(\"DATASET 3: Smart Home Energy Consumption\")\n",\n        "print(\"=\"*80)\n",\n        "\n",\n        "# Load the preprocessed data\n",\n        "df3 = pd.read_csv('preprocessed_smart_home_energy.csv')\n",\n        "print(f\"\\nDataset shape: {df3.shape}\")\n",\n        "print(f\"\\nColumns: {df3.columns.tolist()}\")\n",\n        "\n",\n        "# Prepare features and target\n",\n        "if 'datetime' in df3.columns:\n",\n        "    df3 = df3.drop('datetime', axis=1)\n",\n        "elif 'Unnamed: 0' in df3.columns:\n",\n        "    df3 = df3.drop('Unnamed: 0', axis=1)\n",\n        "\n",\n        "# Define target variable (Energy Consumption (kWh))\n",\n        "target_col_3 = 'Energy Consumption (kWh)'\n",\n        "X3 = df3.drop(target_col_3, axis=1)\n",\n        "y3 = df3[target_col_3]\n",\n        "\n",\n        "# Remove any non-numeric columns\n",\n        "X3 = X3.select_dtypes(include=[np.number])\n",\n        "\n",\n        "print(f\"\\nFeatures shape: {X3.shape}\")\n",\n        "print(f\"Target shape: {y3.shape}\")\n",\n        "\n",\n        "# Split the data\n",\n        "X3_train, X3_test, y3_train, y3_test = train_test_split(\n",\n        "    X3, y3, test_size=0.2, random_state=42\n",\n        ")\n",\n        "\n",\n        "print(f\"\\nTraining set size: {X3_train.shape[0]}\")\n",\n        "print(f\"Test set size: {X3_test.shape[0]}\")"\n      ],\n      "metadata": {\n        "id": "dataset3_load"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Baseline Random Forest Model\n",\n        "print(\"\\n--- Training Baseline Random Forest Model ---\")\n",\n        "start_time = time.time()\n",\n        "\n",\n        "rf_baseline_3 = RandomForestRegressor(\n",\n        "    n_estimators=100,\n",\n        "    random_state=42,\n",\n        "    n_jobs=-1\n",\n        ")\n",\n        "rf_baseline_3.fit(X3_train, y3_train)\n",\n        "\n",\n        "baseline_time_3 = time.time() - start_time\n",\n        "print(f\"Baseline model training time: {baseline_time_3:.2f} seconds\")\n",\n        "\n",\n        "# Evaluate baseline model\n",\n        "baseline_results_3 = evaluate_model(rf_baseline_3, X3_test, y3_test, \"Dataset 3 - Baseline\")\n",\n        "\n",\n        "# Plot predictions\n",\n        "plot_predictions(y3_test, baseline_results_3['predictions'], \"Dataset 3 - Baseline\")\n",\n        "\n",\n        "# Plot feature importance\n",\n        "plot_feature_importance(rf_baseline_3, X3.columns, \"Dataset 3 - Baseline\")"\n      ],\n      "metadata": {\n        "id": "dataset3_baseline"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Hyperparameter Tuning with RandomizedSearchCV\n",\n        "print(\"\\n--- Hyperparameter Tuning (RandomizedSearchCV) ---\")\n",\n        "\n",\n        "# Define parameter grid\n",\n        "param_dist_3 = {\n",\n        "    'n_estimators': [100, 200, 300, 500],\n",\n        "    'max_depth': [10, 20, 30, 40, None],\n",\n        "    'min_samples_split': [2, 5, 10],\n",\n        "    'min_samples_leaf': [1, 2, 4],\n",\n        "    'max_features': ['sqrt', 'log2', None],\n",\n        "    'bootstrap': [True, False]\n",\n        "}\n",\n        "\n",\n        "# Create base model\n",\n        "rf_random_3 = RandomForestRegressor(random_state=42, n_jobs=-1)\n",\n        "\n",\n        "# Randomized search\n",\n        "random_search_3 = RandomizedSearchCV(\n",\n        "    estimator=rf_random_3,\n",\n        "    param_distributions=param_dist_3,\n",\n        "    n_iter=50,\n",\n        "    cv=3,\n",\n        "    verbose=2,\n",\n        "    random_state=42,\n",\n        "    n_jobs=-1,\n",\n        "    scoring='neg_mean_squared_error'\n",\n        ")\n",\n        "\n",\n        "start_time = time.time()\n",\n        "random_search_3.fit(X3_train, y3_train)\n",\n        "tuning_time_3 = time.time() - start_time\n",\n        "\n",\n        "print(f\"\\nHyperparameter tuning time: {tuning_time_3:.2f} seconds\")\n",\n        "print(f\"\\nBest parameters: {random_search_3.best_params_}\")\n",\n        "print(f\"Best cross-validation score (neg MSE): {random_search_3.best_score_:.4f}\")"\n      ],\n      "metadata": {\n        "id": "dataset3_tuning"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Evaluate tuned model\n",\n        "print(\"\\n--- Evaluating Tuned Model ---\")\n",\n        "rf_tuned_3 = random_search_3.best_estimator_\n",\n        "\n",\n        "tuned_results_3 = evaluate_model(rf_tuned_3, X3_test, y3_test, \"Dataset 3 - Tuned\")\n",\n        "\n",\n        "# Plot predictions\n",\n        "plot_predictions(y3_test, tuned_results_3['predictions'], \"Dataset 3 - Tuned\")\n",\n        "\n",\n        "# Plot feature importance\n",\n        "plot_feature_importance(rf_tuned_3, X3.columns, \"Dataset 3 - Tuned\")\n",\n        "\n",\n        "# Compare baseline vs tuned\n",\n        "print(\"\\n--- Baseline vs Tuned Comparison (Dataset 3) ---\")\n",\n        "comparison_df_3 = pd.DataFrame({\n",\n        "    'Metric': ['MSE', 'RMSE', 'MAE', 'RÂ²'],\n",\n        "    'Baseline': [baseline_results_3['mse'], baseline_results_3['rmse'], \n",\n        "                 baseline_results_3['mae'], baseline_results_3['r2']],\n",\n        "    'Tuned': [tuned_results_3['mse'], tuned_results_3['rmse'], \n",\n        "              tuned_results_3['mae'], tuned_results_3['r2']]\n",\n        "})\n",\n        "comparison_df_3['Improvement (%)'] = ((comparison_df_3['Baseline'] - comparison_df_3['Tuned']) / \n",\n        "                                       comparison_df_3['Baseline'] * 100)\n",\n        "print(comparison_df_3.to_string(index=False))"\n      ],\n      "metadata": {\n        "id": "dataset3_eval"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "markdown",\n      "source": [\n        "## Final Summary: All Datasets Comparison"\n      ],\n      "metadata": {\n        "id": "summary_header"\n      }\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Create comprehensive comparison\n",\n        "print(\"\\n\" + \"=\"*80)\n",\n        "print(\"FINAL SUMMARY: ALL DATASETS COMPARISON\")\n",\n        "print(\"=\"*80)\n",\n        "\n",\n        "summary_df = pd.DataFrame({\n",\n        "    'Dataset': ['Household Power (Baseline)', 'Household Power (Tuned)',\n",\n        "                'Appliances Energy (Baseline)', 'Appliances Energy (Tuned)',\n",\n        "                'Smart Home Energy (Baseline)', 'Smart Home Energy (Tuned)'],\n",\n        "    'RMSE': [baseline_results_1['rmse'], tuned_results_1['rmse'],\n",\n        "             baseline_results_2['rmse'], tuned_results_2['rmse'],\n",\n        "             baseline_results_3['rmse'], tuned_results_3['rmse']],\n",\n        "    'MAE': [baseline_results_1['mae'], tuned_results_1['mae'],\n",\n        "            baseline_results_2['mae'], tuned_results_2['mae'],\n",\n        "            baseline_results_3['mae'], tuned_results_3['mae']],\n",\n        "    'RÂ²': [baseline_results_1['r2'], tuned_results_1['r2'],\n",\n        "           baseline_results_2['r2'], tuned_results_2['r2'],\n",\n        "           baseline_results_3['r2'], tuned_results_3['r2']]\n",\n        "})\n",\n        "\n",\n        "print(\"\\n", summary_df.to_string(index=False))\n",\n        "\n",\n        "# Visualize comparison\n",\n        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",\n        "\n",\n        "# RMSE comparison\n",\n        "axes[0].bar(range(len(summary_df)), summary_df['RMSE'])\n",\n        "axes[0].set_xticks(range(len(summary_df)))\n",\n        "axes[0].set_xticklabels(summary_df['Dataset'], rotation=45, ha='right')\n",\n        "axes[0].set_ylabel('RMSE')\n",\n        "axes[0].set_title('RMSE Comparison')\n",\n        "axes[0].grid(axis='y', alpha=0.3)\n",\n        "\n",\n        "# MAE comparison\n",\n        "axes[1].bar(range(len(summary_df)), summary_df['MAE'])\n",\n        "axes[1].set_xticks(range(len(summary_df)))\n",\n        "axes[1].set_xticklabels(summary_df['Dataset'], rotation=45, ha='right')\n",\n        "axes[1].set_ylabel('MAE')\n",\n        "axes[1].set_title('MAE Comparison')\n",\n        "axes[1].grid(axis='y', alpha=0.3)\n",\n        "\n",\n        "# RÂ² comparison\n",\n        "axes[2].bar(range(len(summary_df)), summary_df['RÂ²'])\n",\n        "axes[2].set_xticks(range(len(summary_df)))\n",\n        "axes[2].set_xticklabels(summary_df['Dataset'], rotation=45, ha='right')\n",\n        "axes[2].set_ylabel('RÂ² Score')\n",\n        "axes[2].set_title('RÂ² Score Comparison')\n",\n        "axes[2].grid(axis='y', alpha=0.3)\n",\n        "\n",\n        "plt.tight_layout()\n",\n        "plt.show()\n",\n        "\n",\n        "print(\"\\n\" + \"=\"*80)\n",\n        "print(\"Random Forest Implementation Complete!\")\n",\n        "print(\"=\"*80)"\n      ],\n      "metadata": {\n        "id": "summary_cell"\n      },\n      "execution_count": null,\n      "outputs": []\n    },\n    {\n      "cell_type": "markdown",\n      "source": [\n        "## Save Best Models"\n      ],\n      "metadata": {\n        "id": "save_header"\n      }\n    },\n    {\n      "cell_type": "code",\n      "source": [\n        "# Save the best models\n",\n        "import joblib\n",\n        "\n",\n        "print(\"\\n--- Saving Best Models ---\")\n",\n        "\n",\n        "joblib.dump(rf_tuned_1, 'rf_household_power_tuned.pkl')\n",\n        "print(\"Saved: rf_household_power_tuned.pkl\")\n",\n        "\n",\n        "joblib.dump(rf_tuned_2, 'rf_appliances_energy_tuned.pkl')\n",\n        "print(\"Saved: rf_appliances_energy_tuned.pkl\")\n",\n        "\n",\n        "joblib.dump(rf_tuned_3, 'rf_smart_home_energy_tuned.pkl')\n",\n        "print(\"Saved: rf_smart_home_energy_tuned.pkl\")\n",\n        "\n",\n        "# Save hyperparameters\n",\n        "hyperparams = {\n",\n        "    'Dataset 1': random_search_1.best_params_,\n",\n        "    'Dataset 2': random_search_2.best_params_,\n",\n        "    'Dataset 3': random_search_3.best_params_\n",\n        "}\n",\n        "\n",\n        "import json\n",\n        "with open('best_hyperparameters.json', 'w') as f:\n",\n        "    json.dump(hyperparams, f, indent=4)\n",\n        "print(\"Saved: best_hyperparameters.json\")\n",\n        "\n",\n        "print(\"\\nAll models and hyperparameters saved successfully!\")"\n      ],\n      "metadata": {\n        "id": "save_cell"\n      },\n      "execution_count": null,\n      "outputs": []\n    }\n  ]\n}